# Attention

两种计算方式

![image-20230215130111245](1/image-20230215130111245.png)

右侧+表示，将两个向量拼接起来。



![image-20230215133137425](1/image-20230215133137425.png)



![image-20230215133254793](1/image-20230215133254793.png)



根据注意力分数抽取重要信息，注意力分数大的权重大

![image-20230215134258942](1/image-20230215134258942.png)

以上便是从一整个Sequence得到b1的过程。



b2的生成过程和b1类似

![image-20230215134821483](1/image-20230215134821483.png)



![image-20230215163313296](1/image-20230215163313296.png)



![image-20230215163728745](1/image-20230215163728745.png)



![image-20230215163841482](1/image-20230215163841482.png)



![image-20230215164556885](1/image-20230215164556885.png)



![image-20230215164749697](1/image-20230215164749697.png)



![image-20230215165339087](1/image-20230215165339087.png)



![image-20230215170453267](1/image-20230215170453267.png)



![image-20230215171106051](1/image-20230215171106051.png)



![image-20230215171930689](1/image-20230215171930689.png)



![image-20230215172023728](1/image-20230215172023728.png)



![image-20230215173506130](1/image-20230215173506130.png)

RNN不支持并行，self-attention并行性很好