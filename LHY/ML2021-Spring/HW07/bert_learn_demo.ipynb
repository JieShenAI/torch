{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f727b6d4",
   "metadata": {},
   "source": [
    "# Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "130a2d56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:32:11.757576Z",
     "start_time": "2023-03-15T02:32:11.688759Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HW07.ipynb\n",
      "HW07.pdf\n",
      "bert_learn_demo.ipynb\n",
      "data\n",
      "ml2021-spring-hw7.zip\n",
      "test.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f169f40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:32:11.773533Z",
     "start_time": "2023-03-15T02:32:11.760569Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !tar -zxvf ml2021-spring-hw7.zip -C data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "467ac0c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:32:11.789502Z",
     "start_time": "2023-03-15T02:32:11.776528Z"
    }
   },
   "outputs": [],
   "source": [
    "def prints(arr):\n",
    "    if isinstance(arr, torch.Tensor) or isinstance(arr, np.ndarray):\n",
    "        print(arr, arr.shape)\n",
    "        return\n",
    "    if isinstance(arr, list) or isinstance(arr, tuple):\n",
    "        print(arr, len(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d8a9d73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:32:14.905324Z",
     "start_time": "2023-03-15T02:32:11.793480Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "from transformers import AdamW, BertForQuestionAnswering, BertTokenizerFast\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "def same_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "                \n",
    "same_seeds(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c28eb13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:32:19.539770Z",
     "start_time": "2023-03-15T02:32:14.908315Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForQuestionAnswering: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForQuestionAnswering.from_pretrained(\"bert-base-chinese\").to(device)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afceeaee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:32:19.695148Z",
     "start_time": "2023-03-15T02:32:19.541685Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"questions\": [\n",
      "        {\n",
      "            \"id\": 0,\n",
      "            \"paragraph_id\": 3884,\n",
      "            \"question_text\": \"缇呴Μ鏁欑殗鍒╁ェ涓変笘鍦�800骞存�ｅ紡鍔犲啎瑾扮偤缇呴Μ浜虹殑鐨囧笣?\",\n",
      "            \"answer_text\": \"鏌ョ悊澶у笣\",\n",
      "            \"answer_start\": 141,\n",
      "            \"answer_end\": 144\n",
      "        },\n"
     ]
    }
   ],
   "source": [
    "!head data/hw7_train.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e9b84a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:32:20.011629Z",
     "start_time": "2023-03-15T02:32:19.699150Z"
    }
   },
   "outputs": [],
   "source": [
    "data_home = \"data\"\n",
    "import os\n",
    "def read_data(file, home=data_home):\n",
    "    with open(os.path.join(home,file), 'r', encoding=\"utf-8\") as reader:\n",
    "        data = json.load(reader)\n",
    "    return data[\"questions\"], data[\"paragraphs\"]\n",
    "\n",
    "\n",
    "train_questions, train_paragraphs = read_data(\"hw7_train.json\")\n",
    "dev_questions, dev_paragraphs = read_data(\"hw7_dev.json\")\n",
    "test_questions, test_paragraphs = read_data(\"hw7_test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1912fcd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:32:20.027530Z",
     "start_time": "2023-03-15T02:32:20.014531Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 0,\n",
       "  'paragraph_id': 538,\n",
       "  'question_text': '哪一個訓練中心的設備被使用來訓練網絡城與媒體城勞工的未來知識?',\n",
       "  'answer_text': '杜拜知識村',\n",
       "  'answer_start': 312,\n",
       "  'answer_end': 316},\n",
       " {'id': 1,\n",
       "  'paragraph_id': 281,\n",
       "  'question_text': '南加大單一科系所收到的捐贈最高紀錄是多少?',\n",
       "  'answer_text': '3500萬美元',\n",
       "  'answer_start': 634,\n",
       "  'answer_end': 640},\n",
       " {'id': 2,\n",
       "  'paragraph_id': 953,\n",
       "  'question_text': '梅艷芳以哪一部電影獲得金馬獎?',\n",
       "  'answer_text': '《胭脂扣》',\n",
       "  'answer_start': 426,\n",
       "  'answer_end': 430}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_questions[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78a0effb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:32:20.042458Z",
     "start_time": "2023-03-15T02:32:20.029498Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26936,\n",
       " {'id': 0,\n",
       "  'paragraph_id': 3884,\n",
       "  'question_text': '羅馬教皇利奧三世在800年正式加冕誰為羅馬人的皇帝?',\n",
       "  'answer_text': '查理大帝',\n",
       "  'answer_start': 141,\n",
       "  'answer_end': 144})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_questions), train_questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2657c7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:32:20.073385Z",
     "start_time": "2023-03-15T02:32:20.049441Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3524, 1000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_questions), len(dev_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89763cda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:32:20.088336Z",
     "start_time": "2023-03-15T02:32:20.076366Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('梅艷芳，1963年10月10日－2003年12月30日，香港出生，香港歌影壇巨星，人稱「樂壇大姐大」和「舞台女王」。她的形象百變，更是華語樂壇首位在同一場演唱會換多套歌衫以及在每張專輯的封面和音樂影片都有不同造型的歌手，因此亦有「百變女歌神」之稱。梅艷芳的影響力不只在演藝界，她更代表了整個香港及華人社會，她去世後被傳媒廣泛稱為「香港之女」。梅艷芳跟徐小鳳和蔡琴一樣，是華人樂壇少有的女低音。梅艷芳自小與家人在荔園、啟德遊樂場賣唱。1982年，在無線電視及華星唱片合辦的第一屆新秀歌唱大賽中以一曲〈風的季節〉勝出，星途由此展開。在事業早期，歌曲路線已是冶艷前衛，舞台衣著華麗大膽，風格千變萬化。梅艷芳在1985年－1989年十大勁歌金曲頒獎典禮當中連續五屆奪得最受歡迎女歌星並於1989年叱吒樂壇流行榜頒獎典禮奪得叱吒樂壇女歌手金獎。而於1985年推出的專輯《壞女孩》銷量更達14白金，奠定了她在歌壇的地位。她在電影界也獲得多項大獎，1987年更以電影《胭脂扣》一舉拿下金馬獎、香港電影金像獎及亞太影展最佳女主角后冠，演技備受肯定。她同時也是華人女歌手中演唱會場次的最高紀錄保持者，共計292場。',\n",
       " '《胭脂扣》')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_paragraphs[953], dev_paragraphs[953][426:431]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227e97af",
   "metadata": {},
   "source": [
    "## tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "747174f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:32:20.104325Z",
     "start_time": "2023-03-15T02:32:20.092323Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'paragraph_id', 'question_text', 'answer_text', 'answer_start', 'answer_end'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_questions[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd373f1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:32:23.828332Z",
     "start_time": "2023-03-15T02:32:20.107286Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (570 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Tokenize questions and paragraphs separately\n",
    "# 「add_special_tokens」 is set to False since special tokens will be added \n",
    "    # when tokenized questions and paragraphs are combined in datset __getitem__ \n",
    "\n",
    "train_questions_tokenized = tokenizer([train_question[\"question_text\"] \n",
    "       for train_question in train_questions], add_special_tokens=False)\n",
    "dev_questions_tokenized = tokenizer([dev_question[\"question_text\"] \n",
    "       for dev_question in dev_questions], add_special_tokens=False)\n",
    "test_questions_tokenized = tokenizer([test_question[\"question_text\"] \n",
    "        for test_question in test_questions], add_special_tokens=False) \n",
    "\n",
    "# add_special_tokens = False, 导致ids 没有 101 和 102\n",
    "train_paragraphs_tokenized = tokenizer(train_paragraphs, \n",
    "                                       add_special_tokens=False)\n",
    "dev_paragraphs_tokenized = tokenizer(dev_paragraphs, \n",
    "                                     add_special_tokens=False)\n",
    "test_paragraphs_tokenized = tokenizer(test_paragraphs, \n",
    "                                      add_special_tokens=False)\n",
    "\n",
    "# You can safely ignore the warning message as tokenized sequences will be futher processed in datset __getitem__ before passing to model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca12a56f",
   "metadata": {},
   "source": [
    "### add_special_tokens=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97705cbb",
   "metadata": {},
   "source": [
    "`train_questions_tokenized`、 `train_paragraphs_tokenized`的ids 不是101开头、102结尾, 是因为设置了`add_special_tokens=False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcedd005",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:32:23.844290Z",
     "start_time": "2023-03-15T02:32:23.831325Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5397, 7679, 3136, 4640, 1164, 1953, 676, 686, 1762, 8280, 2399, 3633, 2466, 1217, 1089, 6306, 4158, 5397, 7679, 782, 4638, 4640, 2370, 136]\n"
     ]
    }
   ],
   "source": [
    "print(train_questions_tokenized[0].ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c560b19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:32:23.860246Z",
     "start_time": "2023-03-15T02:32:23.849276Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2010年引進的廣州快速公交運輸系統，屬世界第二大快速公交系統，日常載客量可達100萬人次，高峰時期每小時單向客流高達26900人次，僅次於波哥大的快速交通系統，平均每10秒鐘就有一輛巴士，每輛巴士單向行駛350小時。包括橋樑在內的站台是世界最長的州快速公交運輸系統站台，長達260米。目前廣州市區的計程車和公共汽車主要使用液化石油氣作燃料，部分公共汽車更使用油電、氣電混合動力技術。2012年底開始投放液化天然氣燃料的公共汽車，2014年6月開始投放液化天然氣插電式混合動力公共汽車，以取代液化石油氣公共汽車。2007年1月16日，廣州市政府全面禁止在市區內駕駛摩托車。違反禁令的機動車將會予以沒收。廣州市交通局聲稱禁令的施行，使得交通擁擠問題和車禍大幅減少。廣州白雲國際機場位於白雲區與花都區交界，2004年8月5日正式投入運營，屬中國交通情況第二繁忙的機場。該機場取代了原先位於市中心的無法滿足日益增長航空需求的舊機場。目前機場有三條飛機跑道，成為國內第三個擁有三跑道的民航機場。比鄰近的香港國際機場第三跑道預計的2023年落成早8年。'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_paragraphs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ae7ed5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:32:23.876204Z",
     "start_time": "2023-03-15T02:32:23.865258Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 8166, 2399, 2471, 6868, 4638, 2451, 2336, 2571, 6862, 1062, 769, 6880, 6745, 5143, 5186, 8024, 2253, 686, 4518, 5018, 753, 1920, 2571, 6862, 1062, 769, 5143, 5186, 8024, 3189, 2382, 6734, 2145, 7030, 1377, 6888, 8135, 5857, 782, 3613, 8024, 7770, 2292, 3229, 3309, 102]\n"
     ]
    }
   ],
   "source": [
    "token_test = tokenizer(train_paragraphs[0][:50])\n",
    "print(token_test[0].ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7fff811c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:32:23.892160Z",
     "start_time": "2023-03-15T02:32:23.879196Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8166, 2399, 2471, 6868, 4638, 2451, 2336, 2571, 6862, 1062, 769, 6880, 6745, 5143, 5186, 8024, 2253, 686, 4518, 5018, 753, 1920, 2571, 6862, 1062, 769, 5143, 5186, 8024, 3189, 2382, 6734, 2145, 7030, 1377, 6888, 8135, 5857, 782, 3613, 8024, 7770, 2292, 3229, 3309]\n"
     ]
    }
   ],
   "source": [
    "token_test2 = tokenizer(train_paragraphs[0][:50],add_special_tokens=False)\n",
    "print(token_test2[0].ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885820f2",
   "metadata": {},
   "source": [
    "### tokenize demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "080ea831",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:32:23.908119Z",
     "start_time": "2023-03-15T02:32:23.895163Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_list = dev_paragraphs[953].split('。')\n",
    "len(s_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04b1d1b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:32:23.924076Z",
     "start_time": "2023-03-15T02:32:23.911111Z"
    }
   },
   "outputs": [],
   "source": [
    "temp = tokenizer(s_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d46be2f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:32:23.939036Z",
     "start_time": "2023-03-15T02:32:23.926070Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db761b6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:32:23.954996Z",
     "start_time": "2023-03-15T02:32:23.942028Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [101, 3449, 5684, 5710, 8024, 9155, 2399, 8108, 3299, 8108, 3189, 8025, 8263, 2399, 8110, 3299, 8114, 3189, 8024, 7676, 3949, 1139, 4495, 8024, 7676, 3949, 3625, 2512, 1883, 2342, 3215, 8024, 782, 4935, 519, 3556, 1883, 1920, 1995, 1920, 520, 1469, 519, 5659, 1378, 1957, 4374, 520, 102] 梅艷芳，1963年10月10日－2003年12月30日，香港出生，香港歌影壇巨星，人稱「樂壇大姐大」和「舞台女王」\n",
      "49 57\n",
      "--------------------decode--------------------\n",
      "[CLS] 梅 艷 芳 ， 1963 年 10 月 10 日 － 2003 年 12 月 30 日 ， 香 港 出 生 ， 香 港 歌 影 壇 巨 星 ， 人 稱 「 樂 壇 大 姐 大 」 和 「 舞 台 女 王 」 [SEP]\n"
     ]
    }
   ],
   "source": [
    "for idx, (token_id, s) in enumerate(zip(temp['input_ids'], s_list)):  \n",
    "    print(idx, token_id, s)\n",
    "    print(len(token_id),len(s))\n",
    "    print('-' * 20 + 'decode' + '-' * 20)\n",
    "    print(tokenizer.decode(token_id))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f24e8e",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### char_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3eb3674",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:32:23.969952Z",
     "start_time": "2023-03-15T02:32:23.957986Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (4, 8), (8, 9), (9, 11), (11, 12), (12, 14), (14, 15), (15, 16), (16, 20), (20, 21), (21, 23), (23, 24), (24, 26), (26, 27), (27, 28), (28, 29), (29, 30), (30, 31), (31, 32), (32, 33), (33, 34), (34, 35), (35, 36), (36, 37), (37, 38), (38, 39), (39, 40), (40, 41), (41, 42), (42, 43), (43, 44), (44, 45), (45, 46), (46, 47), (47, 48), (48, 49), (49, 50), (50, 51), (51, 52), (52, 53), (53, 54), (54, 55), (55, 56), (56, 57), (0, 0)]\n"
     ]
    }
   ],
   "source": [
    "print(temp[0].offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f97fe9ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:32:23.985910Z",
     "start_time": "2023-03-15T02:32:23.971948Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '梅', '艷', '芳', '，', '1963', '年', '10', '月', '10', '日', '－', '2003', '年', '12', '月', '30', '日', '，', '香', '港', '出', '生', '，', '香', '港', '歌', '影', '壇', '巨', '星', '，', '人', '稱', '「', '樂', '壇', '大', '姐', '大', '」', '和', '「', '舞', '台', '女', '王', '」', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(temp[0].tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c07d1504",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:32:24.001868Z",
     "start_time": "2023-03-15T02:32:23.988902Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 0),(1, 0, 1),(2, 1, 2),(3, 2, 3),(4, 3, 4),(5, 4, 8),(6, 8, 9),(7, 9, 11),(8, 11, 12),(9, 12, 14),(10, 14, 15),(11, 15, 16),(12, 16, 20),(13, 20, 21),(14, 21, 23),(15, 23, 24),(16, 24, 26),(17, 26, 27),(18, 27, 28),(19, 28, 29),(20, 29, 30),(21, 30, 31),(22, 31, 32),(23, 32, 33),(24, 33, 34),(25, 34, 35),(26, 35, 36),(27, 36, 37),(28, 37, 38),(29, 38, 39),(30, 39, 40),(31, 40, 41),(32, 41, 42),(33, 42, 43),(34, 43, 44),(35, 44, 45),(36, 45, 46),(37, 46, 47),(38, 47, 48),(39, 48, 49),(40, 49, 50),(41, 50, 51),(42, 51, 52),(43, 52, 53),(44, 53, 54),(45, 54, 55),(46, 55, 56),(47, 56, 57),(48, 0, 0),"
     ]
    }
   ],
   "source": [
    "for idx,(v1,v2) in enumerate(temp[0].offsets):\n",
    "    print((idx, v1, v2), end=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8c34e6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:32:24.017825Z",
     "start_time": "2023-03-15T02:32:24.004859Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9, 18, 47, None, "
     ]
    }
   ],
   "source": [
    "for i in [13,27,56,57]:\n",
    "    print(temp[0].char_to_token(i), end=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ec79aaf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:32:24.033782Z",
     "start_time": "2023-03-15T02:32:24.019820Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9, 18, 47, None, "
     ]
    }
   ],
   "source": [
    "for i in [13,27,56,57]:\n",
    "    print(temp.char_to_token(i), end=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a90779dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:32:24.048741Z",
     "start_time": "2023-03-15T02:32:24.036774Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14, 28, 57, 58, "
     ]
    }
   ],
   "source": [
    "for i in [13,27,56,57]:\n",
    "    print(temp[1].char_to_token(i), end=', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc38ad8",
   "metadata": {},
   "source": [
    "#### idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf6d9d18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:32:24.079660Z",
     "start_time": "2023-03-15T02:32:24.058715Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=49, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15201164",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:32:24.095616Z",
     "start_time": "2023-03-15T02:32:24.084646Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 3449, 5684, 5710, 8024, 9155, 2399, 8108, 3299, 8108, 3189, 8025, 8263, 2399, 8110, 3299, 8114, 3189, 8024, 7676, 3949, 1139, 4495, 8024, 7676, 3949, 3625, 2512, 1883, 2342, 3215, 8024, 782, 4935, 519, 3556, 1883, 1920, 1995, 1920, 520, 1469, 519, 5659, 1378, 1957, 4374, 520, 102]\n"
     ]
    }
   ],
   "source": [
    "print(temp[0].ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "964c5a4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:32:24.111575Z",
     "start_time": "2023-03-15T02:32:24.100606Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(temp[0].type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ecc7e545",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:32:24.126533Z",
     "start_time": "2023-03-15T02:32:24.115564Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '梅', '艷', '芳', '，', '1963', '年', '10', '月', '10', '日', '－', '2003', '年', '12', '月', '30', '日', '，', '香', '港', '出', '生', '，', '香', '港', '歌', '影', '壇', '巨', '星', '，', '人', '稱', '「', '樂', '壇', '大', '姐', '大', '」', '和', '「', '舞', '台', '女', '王', '」', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(temp[0].tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d71669bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:32:24.142503Z",
     "start_time": "2023-03-15T02:32:24.128530Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (4, 8), (8, 9), (9, 11), (11, 12), (12, 14), (14, 15), (15, 16), (16, 20), (20, 21), (21, 23), (23, 24), (24, 26), (26, 27), (27, 28), (28, 29), (29, 30), (30, 31), (31, 32), (32, 33), (33, 34), (34, 35), (35, 36), (36, 37), (37, 38), (38, 39), (39, 40), (40, 41), (41, 42), (42, 43), (43, 44), (44, 45), (45, 46), (46, 47), (47, 48), (48, 49), (49, 50), (50, 51), (51, 52), (52, 53), (53, 54), (54, 55), (55, 56), (56, 57), (0, 0)]\n"
     ]
    }
   ],
   "source": [
    "print(temp[0].offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0dec0854",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:32:24.157451Z",
     "start_time": "2023-03-15T02:32:24.144485Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] 49\n"
     ]
    }
   ],
   "source": [
    "print(temp[0].attention_mask, len(temp[0].attention_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "460ce083",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:32:24.172415Z",
     "start_time": "2023-03-15T02:32:24.160444Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(temp[0].special_tokens_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a9723e59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:32:24.187371Z",
     "start_time": "2023-03-15T02:32:24.175404Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(temp[0].overflowing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b93cf8",
   "metadata": {},
   "source": [
    "overflowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8e6445d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:32:25.973726Z",
     "start_time": "2023-03-15T02:32:24.189365Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "s = \"梅艷芳，1963年10月10日－2003年12月30日，香港出生，香港歌影壇巨星，人稱「樂壇大姐大」和「舞台女王」。她的形象百變，更是華語樂壇首位在同一場演唱會換多套歌衫以及在每張專輯的封面和音樂影片都有不同造型的歌手，因此亦有「百變女歌神」之稱。梅艷芳的影響力不只在演藝界，她更代表了整個香港及華人社會，她去世後被傳媒廣泛稱為「香港之女」。梅艷芳跟徐小鳳和蔡琴一樣，是華人樂壇少有的女低音。梅艷芳自小與家人在荔園、啟德遊樂場賣唱。1982年，在無線電視及華星唱片合辦的第一屆新秀歌唱大賽中以一曲〈風的季節〉勝出，星途由此展開。在事業早期，歌曲路線已是冶艷前衛，舞台衣著華麗大膽，風格千變萬化。梅艷芳在1985年－1989年十大勁歌金曲頒獎典禮當中連續五屆奪得最受歡迎女歌星並於1989年叱吒樂壇流行榜頒獎典禮奪得叱吒樂壇女歌手金獎。而於1985年推出的專輯《壞女孩》銷量更達14白金，奠定了她在歌壇的地位。她在電影界也獲得多項大獎，1987年更以電影《胭脂扣》一舉拿下金馬獎、香港電影金像獎及亞太影展最佳女主角后冠，演技備受肯定。她同時也是華人女歌手中演唱會場次的最高紀錄保持者，共計292場。\"\n",
    "\n",
    "over_demo = tokenizer([s, s*2, s*3, s*1100])\n",
    "print(over_demo.keys())\n",
    "print(over_demo[3].overflowing) # 不知道overflowing是什么"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3918ed2",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b9bb14e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:32:25.989760Z",
     "start_time": "2023-03-15T02:32:25.977704Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3884"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_paragraph_id = train_questions[0][\"paragraph_id\"]\n",
    "q_paragraph_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "25fead53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:32:26.004979Z",
     "start_time": "2023-03-15T02:32:25.991982Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=680, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_paragraphs_tokenized[q_paragraph_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4193f2c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:32:26.020151Z",
     "start_time": "2023-03-15T02:32:26.006929Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# tokenized_paragraphs[question[\"paragraph_id\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22052988",
   "metadata": {},
   "source": [
    "### Dataset Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe84cff8",
   "metadata": {},
   "source": [
    "拿到训练集的第一个问题和第一个答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "51972230",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:32:26.035797Z",
     "start_time": "2023-03-15T02:32:26.022397Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'paragraph_id': 3884,\n",
       " 'question_text': '羅馬教皇利奧三世在800年正式加冕誰為羅馬人的皇帝?',\n",
       " 'answer_text': '查理大帝',\n",
       " 'answer_start': 141,\n",
       " 'answer_end': 144}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "297a7eea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:32:26.051132Z",
     "start_time": "2023-03-15T02:32:26.037796Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('利奧三世開創的伊蘇里亞王朝在8世紀末期走上了末路，隨後統治帝國的一群無能皇帝進一步加深了災難局面。這其中最著名的是伊琳娜女皇，她弄瞎了作為法定繼承人的兒子君士坦丁六世的眼睛，將其關入修道院，自己成為第一個大權獨攬的東羅馬女皇。此舉影響重大，導致羅馬教皇利奧三世在800年把法蘭克國王查理大帝加冕為羅馬人的皇帝，使西方帝國有了與東羅馬帝國分庭抗禮的藉口。此外，皇帝尼基弗魯斯在與多瑙河下游平原的保加利亞第一帝國的普利斯卡戰役中被殺，頭蓋骨更被保加利亞酋長克魯姆做成了酒杯。馬其頓王朝的誕生開創了東羅馬帝國歷史上第二個最輝煌的時期。馬其頓王朝開國皇帝巴西爾一世生於亞美尼亞，幼時全家被多瑙河下游平原的保加利亞第一帝國俘虜，發配到馬其頓去開墾土地。長大後，他成為皇宮馬倌，貌美而多力，受到阿莫利王朝末代皇帝麥可三世的注意和寵愛。麥可任命他為宮廷侍衛長，並於866年把他立為自己的繼承人和共帝。867年，巴西爾發覺自己有失寵的跡象，於是在9月23日晚上發動了政變，他先用手擰彎了皇帝寢室的門閂，然後在半夜帶著親信殺入皇帝睡房，迅速制服衛兵，並殺掉了麥可三世。雖然皇位為篡奪而來，但巴西爾一世很快以自己的英明行為讓大家刮目相看。他在軍事上的勝利使其躋身於東羅馬帝國帝國最偉大的軍事家之列。他嚴格貫徹席哈克略王朝時開始的軍事制改革，在巴爾幹半島建立新軍事州，向這些地區遷入新移民，並憑藉不斷增強的君主制國的國力鞏固國防建設，不僅在巴爾幹半島的多瑙河沿岸北部設立邊境要塞，成功阻擋了斯拉夫人南下，而且在小亞細亞擴充軍隊並反擊了阿拉伯人的侵略，在義大利南部，也收復了原屬於東羅馬帝國的領地。',\n",
       " '查理大帝')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_paragraphs[3884], train_paragraphs[3884][141:144+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4298bb1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:32:26.081761Z",
     "start_time": "2023-03-15T02:32:26.053410Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(139, 142)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_paragraphs_tokenized[3884].char_to_token(141), \\\n",
    "train_paragraphs_tokenized[3884].char_to_token(144)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ba5a9a31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:32:26.097629Z",
     "start_time": "2023-03-15T02:32:26.083668Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['利', '奧', '三', '世', '開', '創', '的', '伊', '蘇', '里', '亞', '王', '朝', '在', '8', '世', '紀', '末', '期', '走', '上', '了', '末', '路', '，', '隨', '後', '統', '治', '帝', '國', '的', '一', '群', '無', '能', '皇', '帝', '進', '一', '步', '加', '深', '了', '災', '難', '局', '面', '。', '這', '其', '中', '最', '著', '名', '的', '是', '伊', '琳', '娜', '女', '皇', '，', '她', '弄', '瞎', '了', '作', '為', '法', '定', '繼', '承', '人', '的', '兒', '子', '君', '士', '坦', '丁', '六', '世', '的', '眼', '睛', '，', '將', '其', '關', '入', '修', '道', '院', '，', '自', '己', '成', '為', '第', '一', '個', '大', '權', '獨', '攬', '的', '東', '羅', '馬', '女', '皇', '。', '此', '舉', '影', '響', '重', '大', '，', '導', '致', '羅', '馬', '教', '皇', '利', '奧', '三', '世', '在', '800', '年', '把', '法', '蘭', '克', '國', '王', '查', '理', '大', '帝', '加', '冕', '為', '羅', '馬', '人', '的', '皇', '帝', '，', '使', '西', '方', '帝', '國', '有', '了', '與', '東', '羅', '馬', '帝', '國', '分', '庭', '抗', '禮', '的', '藉', '口', '。', '此', '外', '，', '皇', '帝', '尼', '基', '弗', '魯', '斯', '在', '與', '多', '瑙', '河', '下', '游', '平', '原', '的', '保', '加', '利', '亞', '第', '一', '帝', '國', '的', '普', '利', '斯', '卡', '戰', '役', '中', '被', '殺', '，', '頭', '蓋', '骨', '更', '被', '保', '加', '利', '亞', '酋', '長', '克', '魯', '姆', '做', '成', '了', '酒', '杯', '。', '馬', '其', '頓', '王', '朝', '的', '誕', '生', '開', '創', '了', '東', '羅', '馬', '帝', '國', '歷', '史', '上', '第', '二', '個', '最', '輝', '煌', '的', '時', '期', '。', '馬', '其', '頓', '王', '朝', '開', '國', '皇', '帝', '巴', '西', '爾', '一', '世', '生', '於', '亞', '美', '尼', '亞', '，', '幼', '時', '全', '家', '被', '多', '瑙', '河', '下', '游', '平', '原', '的', '保', '加', '利', '亞', '第', '一', '帝', '國', '俘', '虜', '，', '發', '配', '到', '馬', '其', '頓', '去', '開', '墾', '土', '地', '。', '長', '大', '後', '，', '他', '成', '為', '皇', '宮', '馬', '倌', '，', '貌', '美', '而', '多', '力', '，', '受', '到', '阿', '莫', '利', '王', '朝', '末', '代', '皇', '帝', '麥', '可', '三', '世', '的', '注', '意', '和', '寵', '愛', '。', '麥', '可', '任', '命', '他', '為', '宮', '廷', '侍', '衛', '長', '，', '並', '於', '866', '年', '把', '他', '立', '為', '自', '己', '的', '繼', '承', '人', '和', '共', '帝', '。', '867', '年', '，', '巴', '西', '爾', '發', '覺', '自', '己', '有', '失', '寵', '的', '跡', '象', '，', '於', '是', '在', '9', '月', '23', '日', '晚', '上', '發', '動', '了', '政', '變', '，', '他', '先', '用', '手', '擰', '彎', '了', '皇', '帝', '寢', '室', '的', '門', '[UNK]', '，', '然', '後', '在', '半', '夜', '帶', '著', '親', '信', '殺', '入', '皇', '帝', '睡', '房', '，', '迅', '速', '制', '服', '衛', '兵', '，', '並', '殺', '掉', '了', '麥', '可', '三', '世', '。', '雖', '然', '皇', '位', '為', '篡', '奪', '而', '來', '，', '但', '巴', '西', '爾', '一', '世', '很', '快', '以', '自', '己', '的', '英', '明', '行', '為', '讓', '大', '家', '刮', '目', '相', '看', '。', '他', '在', '軍', '事', '上', '的', '勝', '利', '使', '其', '躋', '身', '於', '東', '羅', '馬', '帝', '國', '帝', '國', '最', '偉', '大', '的', '軍', '事', '家', '之', '列', '。', '他', '嚴', '格', '貫', '徹', '席', '哈', '克', '略', '王', '朝', '時', '開', '始', '的', '軍', '事', '制', '改', '革', '，', '在', '巴', '爾', '幹', '半', '島', '建', '立', '新', '軍', '事', '州', '，', '向', '這', '些', '地', '區', '遷', '入', '新', '移', '民', '，', '並', '憑', '藉', '不', '斷', '增', '強', '的', '君', '主', '制', '國', '的', '國', '力', '鞏', '固', '國', '防', '建', '設', '，', '不', '僅', '在', '巴', '爾', '幹', '半', '島', '的', '多', '瑙', '河', '沿', '岸', '北', '部', '設', '立', '邊', '境', '要', '塞', '，', '成', '功', '阻', '擋', '了', '斯', '拉', '夫', '人', '南', '下', '，', '而', '且', '在', '小', '亞', '細', '亞', '擴', '充', '軍', '隊', '並', '反', '擊', '了', '阿', '拉', '伯', '人', '的', '侵', '略', '，', '在', '義', '大', '利', '南', '部', '，', '也', '收', '復', '了', '原', '屬', '於', '東', '羅', '馬', '帝', '國', '的', '領', '地', '。']\n"
     ]
    }
   ],
   "source": [
    "print(train_paragraphs_tokenized[3884].tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7d80c1ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:32:26.113724Z",
     "start_time": "2023-03-15T02:32:26.101635Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['查', '理', '大', '帝']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_paragraphs_tokenized[3884].tokens[139:142+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bd5d3714",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:32:26.129736Z",
     "start_time": "2023-03-15T02:32:26.115581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8166, 2399, 2471, 6868, 4638, 2451, 2336, 2571, 6862, 1062, 769, 6880, 6745, 5143, 5186, 8024, 2253, 686, 4518, 5018, 753, 1920, 2571, 6862, 1062, 769, 5143, 5186, 8024, 3189, 2382, 6734, 2145, 7030, 1377, 6888, 8135, 5857, 782, 3613, 8024, 7770, 2292, 3229, 3309, 3680, 2207, 3229, 1606, 1403, 2145, 3837, 7770, 6888, 11023, 8279, 782, 3613, 8024, 1006, 3613, 3176, 3797, 1520, 1920, 4638, 2571, 6862, 769, 6858, 5143, 5186, 8024, 2398, 1772, 3680, 8108, 4907, 7132, 2218, 3300, 671, 6739, 2349, 1894, 8024, 3680, 6739, 2349, 1894, 1606, 1403, 6121, 7691, 8612, 2207, 3229, 511, 1259, 2886, 3578, 3558, 1762, 1058, 4638, 4991, 1378, 3221, 686, 4518, 3297, 7269, 4638, 2336, 2571, 6862, 1062, 769, 6880, 6745, 5143, 5186, 4991, 1378, 8024, 7269, 6888, 9044, 5101, 511, 4680, 1184, 2451, 2336, 2356, 1281, 4638, 6243, 4923, 6722, 1469, 1062, 1066, 3749, 6722, 712, 6206, 886, 4500, 3890, 1265, 4767, 3779, 3706, 868, 4234, 3160, 8024, 6956, 1146, 1062, 1066, 3749, 6722, 3291, 886, 4500, 3779, 7442, 510, 3706, 7442, 3921, 1394, 1240, 1213, 2825, 6123, 511, 8151, 2399, 2419, 7274, 1993, 2832, 3123, 3890, 1265, 1921, 4197, 3706, 4234, 3160, 4638, 1062, 1066, 3749, 6722, 8024, 8127, 2399, 127, 3299, 7274, 1993, 2832, 3123, 3890, 1265, 1921, 4197, 3706, 2991, 7442, 2466, 3921, 1394, 1240, 1213, 1062, 1066, 3749, 6722, 8024, 809, 1357, 807, 3890, 1265, 4767, 3779, 3706, 1062, 1066, 3749, 6722, 511, 8201, 2399, 122, 3299, 8121, 3189, 8024, 2451, 2336, 2356, 3124, 2424, 1059, 7481, 4881, 3632, 1762, 2356, 1281, 1058, 7690, 7691, 3040, 2805, 6722, 511, 6889, 1353, 4881, 808, 4638, 3582, 1240, 6722, 2200, 3298, 750, 809, 3760, 3119, 511, 2451, 2336, 2356, 769, 6858, 2229, 5476, 4935, 4881, 808, 4638, 3177, 6121, 8024, 886, 2533, 769, 6858, 3075, 3089, 1558, 7539, 1469, 6722, 4884, 1920, 2388, 3938, 2208, 511, 2451, 2336, 4635, 7437, 1751, 7396, 3582, 1842, 855, 3176, 4635, 7437, 1281, 5645, 5709, 6963, 1281, 769, 4518, 8024, 8258, 2399, 129, 3299, 126, 3189, 3633, 2466, 2832, 1057, 6880, 4245, 8024, 2253, 704, 1751, 769, 6858, 2658, 3785, 5018, 753, 5246, 2564, 4638, 3582, 1842, 511, 6283, 3582, 1842, 1357, 807, 749, 1333, 1044, 855, 3176, 2356, 704, 2552, 4638, 4192, 3791, 4021, 6639, 3189, 4660, 1872, 7269, 5661, 4958, 7444, 3724, 4638, 5648, 3582, 1842, 511, 4680, 1184, 3582, 1842, 3300, 676, 3454, 7606, 3582, 6651, 6887, 8024, 2768, 4158, 1751, 1058, 5018, 676, 943, 3075, 3300, 676, 6651, 6887, 4638, 3696, 5661, 3582, 1842, 511, 3683, 6973, 6818, 4638, 7676, 3949, 1751, 7396, 3582, 1842, 5018, 676, 6651, 6887, 7521, 6243, 4638, 9707, 8152, 2399, 5862, 2768, 3193, 129, 2399, 511]\n"
     ]
    }
   ],
   "source": [
    "print(train_paragraphs_tokenized[0].ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9871e22d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:32:26.145218Z",
     "start_time": "2023-03-15T02:32:26.132052Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5397, 7679, 3136, 4640, 1164, 1953, 676, 686, 1762, 8280, 2399, 3633, 2466, 1217, 1089, 6306, 4158, 5397, 7679, 782, 4638, 4640, 2370, 136]\n"
     ]
    }
   ],
   "source": [
    "print(train_questions_tokenized[0].ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a78ee487",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:32:26.176690Z",
     "start_time": "2023-03-15T02:32:26.148215Z"
    },
    "code_folding": [
     78,
     99
    ]
   },
   "outputs": [],
   "source": [
    "class QA_Dataset(Dataset):\n",
    "    def __init__(self, split, questions, tokenized_questions, tokenized_paragraphs):\n",
    "        self.split = split\n",
    "        self.questions = questions\n",
    "        self.tokenized_questions = tokenized_questions\n",
    "        self.tokenized_paragraphs = tokenized_paragraphs\n",
    "        self.max_question_len = 40\n",
    "        self.max_paragraph_len = 150\n",
    "        \n",
    "        ##### TODO: Change value of doc_stride #####\n",
    "        # paragraph 切分的分片大小\n",
    "        self.doc_stride = 150\n",
    "\n",
    "        # Input sequence length = [CLS] + question + [SEP] + paragraph + [SEP]\n",
    "        self.max_seq_len = 1 + self.max_question_len + 1 + self.max_paragraph_len + 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        question = self.questions[idx]\n",
    "        tokenized_question = self.tokenized_questions[idx]\n",
    "        tokenized_paragraph = self.tokenized_paragraphs[\n",
    "                                question[\"paragraph_id\"]]\n",
    "\n",
    "        ##### TODO: Preprocessing #####\n",
    "        # Hint: How to prevent model from learning something it should not learn\n",
    "        \n",
    "        # 为什么train 不需要给paragraph分片，但valid和test需要分片 ?\n",
    "        if self.split == \"train\":\n",
    "            # Convert answer's start/end positions in paragraph_text to start/end positions in tokenized_paragraph  \n",
    "            \n",
    "            # answer_start_token 139\n",
    "            answer_start_token = tokenized_paragraph.char_to_token(\n",
    "                question[\"answer_start\"])\n",
    "            # answer_end_token 142\n",
    "            answer_end_token = tokenized_paragraph.char_to_token(\n",
    "                question[\"answer_end\"])\n",
    "\n",
    "            # A single window is obtained \n",
    "                # by slicing the portion of paragraph containing the answer\n",
    "            mid = (answer_start_token + answer_end_token) // 2 # 140\n",
    "            \n",
    "            # 确定初始下标时，要保证：\n",
    "                # answer不能被截掉\n",
    "                # 截掉前面的\n",
    "            paragraph_start = max(\n",
    "                0, \n",
    "                min(\n",
    "                      mid - self.max_paragraph_len // 2, \n",
    "                      len(tokenized_paragraph) - self.max_paragraph_len))\n",
    "            \n",
    "            paragraph_end = paragraph_start + self.max_paragraph_len\n",
    "\n",
    "            # Slice question/paragraph and add special tokens (101: CLS, 102: SEP)\n",
    "            input_ids_question = [101] + \\\n",
    "                tokenized_question.ids[:self.max_question_len] + [102]\n",
    "            # 填上 101, 102，是因为在之前通过设置add_special_tokens=False，\n",
    "                # ids中就没有101,102\n",
    "\n",
    "            # 后一句，不需要101开头，只在尾部添加 102\n",
    "            input_ids_paragraph = tokenized_paragraph.ids[\n",
    "                paragraph_start : paragraph_end] + [102]\n",
    "            print(\"input_ids_paragraph\", input_ids_paragraph)\n",
    "            \n",
    "            # Convert answer's start/end positions in tokenized_paragraph \n",
    "                # to start/end positions in the window  \n",
    "            answer_start_token += len(input_ids_question) - paragraph_start\n",
    "            answer_end_token += len(input_ids_question) - paragraph_start\n",
    "\n",
    "            # Pad sequence and obtain inputs to model \n",
    "            input_ids, token_type_ids, attention_mask = self.padding(\n",
    "                    input_ids_question, input_ids_paragraph)\n",
    "\n",
    "            return torch.tensor(input_ids), torch.tensor(token_type_ids), \\\n",
    "                torch.tensor(attention_mask), answer_start_token, answer_end_token\n",
    "\n",
    "        # Validation/Testing\n",
    "        else:\n",
    "            input_ids_list, token_type_ids_list, attention_mask_list = [], [], []\n",
    "            \n",
    "            # Paragraph is split into several windows, \n",
    "                # each with start positions separated by step \"doc_stride\"\n",
    "            for i in range(0, len(tokenized_paragraph), self.doc_stride):\n",
    "                \n",
    "                # Slice question/paragraph and add special tokens (101: CLS, 102: SEP)\n",
    "                input_ids_question = [101] + tokenized_question.ids[:self.max_question_len] + [102]\n",
    "                input_ids_paragraph = tokenized_paragraph.ids[i : i + self.max_paragraph_len] + [102]\n",
    "                \n",
    "                # Pad sequence and obtain inputs to model\n",
    "                input_ids, token_type_ids, attention_mask = self.padding(input_ids_question, input_ids_paragraph)\n",
    "                \n",
    "                input_ids_list.append(input_ids)\n",
    "                token_type_ids_list.append(token_type_ids)\n",
    "                attention_mask_list.append(attention_mask)\n",
    "            \n",
    "            return torch.tensor(input_ids_list), torch.tensor(token_type_ids_list), \\\n",
    "                        torch.tensor(attention_mask_list)\n",
    "\n",
    "    def padding(self, input_ids_question, input_ids_paragraph):\n",
    "        # Pad zeros if sequence length is shorter than max_seq_len\n",
    "        padding_len = self.max_seq_len - len(input_ids_question) - len(input_ids_paragraph)\n",
    "        # Indices of input sequence tokens in the vocabulary\n",
    "        input_ids = input_ids_question + input_ids_paragraph + [0] * padding_len\n",
    "        # Segment token indices to indicate first and second portions of the inputs. Indices are selected in [0, 1]\n",
    "        token_type_ids = [0] * len(input_ids_question) + [1] * len(input_ids_paragraph) + [0] * padding_len\n",
    "        # Mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]\n",
    "        attention_mask = [1] * (len(input_ids_question) + len(input_ids_paragraph)) + [0] * padding_len\n",
    "        \n",
    "        return input_ids, token_type_ids, attention_mask\n",
    "\n",
    "\n",
    "# train_set = QA_Dataset(\"valid\", train_questions, train_questions_tokenized, train_paragraphs_tokenized)\n",
    "# input_ids_list_, token_type_ids_list_, attention_mask_list = train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "964e4db4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:32:26.192570Z",
     "start_time": "2023-03-15T02:32:26.179572Z"
    },
    "code_folding": [
     0,
     1
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass QA_Dataset(Dataset):\\n    def __init__(self, split, questions, tokenized_questions, tokenized_paragraphs):\\n        self.split = split\\n        self.questions = questions\\n        self.tokenized_questions = tokenized_questions\\n        self.tokenized_paragraphs = tokenized_paragraphs\\n        self.max_question_len = 40\\n        self.max_paragraph_len = 150\\n        \\n        ##### TODO: Change value of doc_stride #####\\n        self.doc_stride = 150\\n\\n        # Input sequence length = [CLS] + question + [SEP] + paragraph + [SEP]\\n        self.max_seq_len = 1 + self.max_question_len + 1 + self.max_paragraph_len + 1\\n\\n    def __len__(self):\\n        return len(self.questions)\\n\\n    def __getitem__(self, idx):\\n        question = self.questions[idx]\\n        tokenized_question = self.tokenized_questions[idx]\\n        tokenized_paragraph = self.tokenized_paragraphs[question[\"paragraph_id\"]]\\n\\n        ##### TODO: Preprocessing #####\\n        # Hint: How to prevent model from learning something it should not learn\\n\\n        if self.split == \"train\":\\n            # Convert answer\\'s start/end positions in paragraph_text to start/end positions in tokenized_paragraph  \\n            answer_start_token = tokenized_paragraph.char_to_token(question[\"answer_start\"])\\n            answer_end_token = tokenized_paragraph.char_to_token(question[\"answer_end\"])\\n\\n            # A single window is obtained \\n                # by slicing the portion of paragraph containing the answer\\n            mid = (answer_start_token + answer_end_token) // 2\\n            paragraph_start = max(0, \\n                                  min(mid - self.max_paragraph_len // 2, \\n                                      len(tokenized_paragraph) - self.max_paragraph_len))\\n            paragraph_end = paragraph_start + self.max_paragraph_len\\n            \\n            # Slice question/paragraph and add special tokens (101: CLS, 102: SEP)\\n            input_ids_question = [101] + tokenized_question.ids[:self.max_question_len] + [102] \\n            input_ids_paragraph = tokenized_paragraph.ids[paragraph_start : paragraph_end] + [102]\\t\\t\\n            \\n            # Convert answer\\'s start/end positions in tokenized_paragraph to start/end positions in the window  \\n            answer_start_token += len(input_ids_question) - paragraph_start\\n            answer_end_token += len(input_ids_question) - paragraph_start\\n            \\n            # Pad sequence and obtain inputs to model \\n            input_ids, token_type_ids, attention_mask = self.padding(\\n                    input_ids_question, input_ids_paragraph)\\n            return torch.tensor(input_ids), torch.tensor(token_type_ids), torch.tensor(attention_mask), answer_start_token, answer_end_token\\n\\n        # Validation/Testing\\n        else:\\n            input_ids_list, token_type_ids_list, attention_mask_list = [], [], []\\n            \\n            # Paragraph is split into several windows, each with start positions separated by step \"doc_stride\"\\n            for i in range(0, len(tokenized_paragraph), self.doc_stride):\\n                \\n                # Slice question/paragraph and add special tokens (101: CLS, 102: SEP)\\n                input_ids_question = [101] + tokenized_question.ids[:self.max_question_len] + [102]\\n                input_ids_paragraph = tokenized_paragraph.ids[i : i + self.max_paragraph_len] + [102]\\n                \\n                # Pad sequence and obtain inputs to model\\n                input_ids, token_type_ids, attention_mask = self.padding(input_ids_question, input_ids_paragraph)\\n                \\n                input_ids_list.append(input_ids)\\n                token_type_ids_list.append(token_type_ids)\\n                attention_mask_list.append(attention_mask)\\n            \\n            return torch.tensor(input_ids_list), torch.tensor(token_type_ids_list),                         torch.tensor(attention_mask_list)\\n\\n    def padding(self, input_ids_question, input_ids_paragraph):\\n        # Pad zeros if sequence length is shorter than max_seq_len\\n        padding_len = self.max_seq_len - len(input_ids_question) - len(input_ids_paragraph)\\n        # Indices of input sequence tokens in the vocabulary\\n        input_ids = input_ids_question + input_ids_paragraph + [0] * padding_len\\n        # Segment token indices to indicate first and second portions of the inputs. Indices are selected in [0, 1]\\n        token_type_ids = [0] * len(input_ids_question) + [1] * len(input_ids_paragraph) + [0] * padding_len\\n        # Mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]\\n        attention_mask = [1] * (len(input_ids_question) + len(input_ids_paragraph)) + [0] * padding_len\\n        \\n        return input_ids, token_type_ids, attention_mask\\n\\ntrain_set = QA_Dataset(\"train\", train_questions, train_questions_tokenized, train_paragraphs_tokenized)\\n'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "class QA_Dataset(Dataset):\n",
    "    def __init__(self, split, questions, tokenized_questions, tokenized_paragraphs):\n",
    "        self.split = split\n",
    "        self.questions = questions\n",
    "        self.tokenized_questions = tokenized_questions\n",
    "        self.tokenized_paragraphs = tokenized_paragraphs\n",
    "        self.max_question_len = 40\n",
    "        self.max_paragraph_len = 150\n",
    "        \n",
    "        ##### TODO: Change value of doc_stride #####\n",
    "        self.doc_stride = 150\n",
    "\n",
    "        # Input sequence length = [CLS] + question + [SEP] + paragraph + [SEP]\n",
    "        self.max_seq_len = 1 + self.max_question_len + 1 + self.max_paragraph_len + 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        question = self.questions[idx]\n",
    "        tokenized_question = self.tokenized_questions[idx]\n",
    "        tokenized_paragraph = self.tokenized_paragraphs[question[\"paragraph_id\"]]\n",
    "\n",
    "        ##### TODO: Preprocessing #####\n",
    "        # Hint: How to prevent model from learning something it should not learn\n",
    "\n",
    "        if self.split == \"train\":\n",
    "            # Convert answer's start/end positions in paragraph_text to start/end positions in tokenized_paragraph  \n",
    "            answer_start_token = tokenized_paragraph.char_to_token(question[\"answer_start\"])\n",
    "            answer_end_token = tokenized_paragraph.char_to_token(question[\"answer_end\"])\n",
    "\n",
    "            # A single window is obtained \n",
    "                # by slicing the portion of paragraph containing the answer\n",
    "            mid = (answer_start_token + answer_end_token) // 2\n",
    "            paragraph_start = max(0, \n",
    "                                  min(mid - self.max_paragraph_len // 2, \n",
    "                                      len(tokenized_paragraph) - self.max_paragraph_len))\n",
    "            paragraph_end = paragraph_start + self.max_paragraph_len\n",
    "            \n",
    "            # Slice question/paragraph and add special tokens (101: CLS, 102: SEP)\n",
    "            input_ids_question = [101] + tokenized_question.ids[:self.max_question_len] + [102] \n",
    "            input_ids_paragraph = tokenized_paragraph.ids[paragraph_start : paragraph_end] + [102]\t\t\n",
    "            \n",
    "            # Convert answer's start/end positions in tokenized_paragraph to start/end positions in the window  \n",
    "            answer_start_token += len(input_ids_question) - paragraph_start\n",
    "            answer_end_token += len(input_ids_question) - paragraph_start\n",
    "            \n",
    "            # Pad sequence and obtain inputs to model \n",
    "            input_ids, token_type_ids, attention_mask = self.padding(\n",
    "                    input_ids_question, input_ids_paragraph)\n",
    "            return torch.tensor(input_ids), torch.tensor(token_type_ids), torch.tensor(attention_mask), answer_start_token, answer_end_token\n",
    "\n",
    "        # Validation/Testing\n",
    "        else:\n",
    "            input_ids_list, token_type_ids_list, attention_mask_list = [], [], []\n",
    "            \n",
    "            # Paragraph is split into several windows, each with start positions separated by step \"doc_stride\"\n",
    "            for i in range(0, len(tokenized_paragraph), self.doc_stride):\n",
    "                \n",
    "                # Slice question/paragraph and add special tokens (101: CLS, 102: SEP)\n",
    "                input_ids_question = [101] + tokenized_question.ids[:self.max_question_len] + [102]\n",
    "                input_ids_paragraph = tokenized_paragraph.ids[i : i + self.max_paragraph_len] + [102]\n",
    "                \n",
    "                # Pad sequence and obtain inputs to model\n",
    "                input_ids, token_type_ids, attention_mask = self.padding(input_ids_question, input_ids_paragraph)\n",
    "                \n",
    "                input_ids_list.append(input_ids)\n",
    "                token_type_ids_list.append(token_type_ids)\n",
    "                attention_mask_list.append(attention_mask)\n",
    "            \n",
    "            return torch.tensor(input_ids_list), torch.tensor(token_type_ids_list), \\\n",
    "                        torch.tensor(attention_mask_list)\n",
    "\n",
    "    def padding(self, input_ids_question, input_ids_paragraph):\n",
    "        # Pad zeros if sequence length is shorter than max_seq_len\n",
    "        padding_len = self.max_seq_len - len(input_ids_question) - len(input_ids_paragraph)\n",
    "        # Indices of input sequence tokens in the vocabulary\n",
    "        input_ids = input_ids_question + input_ids_paragraph + [0] * padding_len\n",
    "        # Segment token indices to indicate first and second portions of the inputs. Indices are selected in [0, 1]\n",
    "        token_type_ids = [0] * len(input_ids_question) + [1] * len(input_ids_paragraph) + [0] * padding_len\n",
    "        # Mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]\n",
    "        attention_mask = [1] * (len(input_ids_question) + len(input_ids_paragraph)) + [0] * padding_len\n",
    "        \n",
    "        return input_ids, token_type_ids, attention_mask\n",
    "\n",
    "train_set = QA_Dataset(\"train\", train_questions, train_questions_tokenized, train_paragraphs_tokenized)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ecd04fbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-15T02:33:08.759326Z",
     "start_time": "2023-03-15T02:33:08.744367Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "train_set = QA_Dataset(\"train\", train_questions, train_questions_tokenized, train_paragraphs_tokenized)\n",
    "dev_set = QA_Dataset(\"dev\", dev_questions, dev_questions_tokenized, dev_paragraphs_tokenized)\n",
    "test_set = QA_Dataset(\"test\", test_questions, test_questions_tokenized, test_paragraphs_tokenized)\n",
    "\n",
    "train_batch_size = 16\n",
    "\n",
    "# Note: Do NOT change batch size of dev_loader / test_loader !\n",
    "# Although batch size=1, it is actually a batch consisting of several windows from the same QA pair\n",
    "train_loader = DataLoader(train_set, batch_size=train_batch_size, shuffle=True, pin_memory=True)\n",
    "dev_loader = DataLoader(dev_set, batch_size=1, shuffle=False, pin_memory=True)\n",
    "test_loader = DataLoader(test_set, batch_size=1, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f9b956",
   "metadata": {},
   "source": [
    "## evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7353c2",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cfb5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 1 \n",
    "validation = True\n",
    "logging_step = 100\n",
    "learning_rate = 1e-4\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "if fp16_training:\n",
    "    model, optimizer, train_loader = accelerator.prepare(model, optimizer, train_loader) \n",
    "\n",
    "model.train()\n",
    "\n",
    "print(\"Start Training ...\")\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    step = 1\n",
    "    train_loss = train_acc = 0\n",
    "    \n",
    "    for data in tqdm(train_loader):\t\n",
    "        # Load all data into GPU\n",
    "        data = [i.to(device) for i in data]\n",
    "        \n",
    "        # Model inputs: input_ids, token_type_ids, attention_mask, start_positions, end_positions (Note: only \"input_ids\" is mandatory)\n",
    "        # Model outputs: start_logits, end_logits, loss (return when start_positions/end_positions are provided)  \n",
    "        output = model(\n",
    "            input_ids=data[0], \n",
    "            token_type_ids=data[1], \n",
    "            attention_mask=data[2], \n",
    "            start_positions=data[3], \n",
    "            end_positions=data[4]\n",
    "        )\n",
    "\n",
    "        # Choose the most probable start position / end position\n",
    "        start_index = torch.argmax(output.start_logits, dim=1)\n",
    "        end_index = torch.argmax(output.end_logits, dim=1)\n",
    "        \n",
    "        # Prediction is correct only if both start_index and end_index are correct\n",
    "        train_acc += ((start_index == data[3]) & (end_index == data[4])).float().mean()\n",
    "        train_loss += output.loss\n",
    "        \n",
    "        if fp16_training:\n",
    "            accelerator.backward(output.loss)\n",
    "        else:\n",
    "            output.loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        step += 1\n",
    "\n",
    "        ##### TODO: Apply linear learning rate decay #####\n",
    "        \n",
    "        # Print training loss and accuracy over past logging step\n",
    "        if step % logging_step == 0:\n",
    "            print(f\"Epoch {epoch + 1} | Step {step} |\n",
    "                  loss = {train_loss.item() / logging_step:.3f}, \n",
    "                  acc = {train_acc / logging_step:.3f}\")\n",
    "            train_loss = train_acc = 0\n",
    "\n",
    "    if validation:\n",
    "        print(\"Evaluating Dev Set ...\")\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            dev_acc = 0\n",
    "            for i, data in enumerate(tqdm(dev_loader)):\n",
    "                output = model(input_ids=data[0].squeeze(dim=0).to(device), token_type_ids=data[1].squeeze(dim=0).to(device),\n",
    "                       attention_mask=data[2].squeeze(dim=0).to(device))\n",
    "                # prediction is correct only if answer text exactly matches\n",
    "                dev_acc += evaluate(data, output) == dev_questions[i][\"answer_text\"]\n",
    "            print(f\"Validation | Epoch {epoch + 1} | acc = {dev_acc / len(dev_loader):.3f}\")\n",
    "        model.train()\n",
    "\n",
    "# Save a model and its configuration file to the directory 「saved_model」 \n",
    "# i.e. there are two files under the direcory 「saved_model」: 「pytorch_model.bin」 and 「config.json」\n",
    "# Saved model can be re-loaded using 「model = BertForQuestionAnswering.from_pretrained(\"saved_model\")」\n",
    "print(\"Saving Model ...\")\n",
    "model_save_dir = \"saved_model\" \n",
    "model.save_pretrained(model_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6928f98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "199.15px",
    "width": "684.162px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "103.186px",
    "left": "364.714px",
    "right": "20px",
    "top": "10px",
    "width": "392.5px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
