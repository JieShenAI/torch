{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "reference: \n",
        "\n",
        "* https://www.kaggle.com/datasets/hassan06/nslkdd/code\n",
        "* https://www.kaggle.com/code/nadagamal3/network-security-attack-classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qZd6l_z5rRoA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset, random_split, ConcatDataset\n",
        "from torch import optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OkWcS-HHmbmV"
      },
      "outputs": [],
      "source": [
        "# project_home = '/content/drive/MyDrive/torch/example/basic/NSL_KDD'\n",
        "project_home = '.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Rp9Ar8smnzhc"
      },
      "outputs": [],
      "source": [
        "# !unzip {project_home}/data/NSL-KDD.zip -d {project_home}/data/"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "模型的基本设置参数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nUNx51WeoHLa"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"train\": f'{project_home}/data/KDDTrain+.txt',\n",
        "    \"test\": f'{project_home}/data/KDDTest+.txt',\n",
        "    \"batch_size\": 128,\n",
        "}\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5Pq6oRZZO51S"
      },
      "source": [
        "## 数据预处理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "columns = (['duration','protocol_type','service','flag','src_bytes','dst_bytes','land','wrong_fragment','urgent','hot',\n",
        "            'num_failed_logins','logged_in','num_compromised','root_shell','su_attempted','num_root','num_file_creations',\n",
        "            'num_shells','num_access_files','num_outbound_cmds','is_host_login','is_guest_login','count','srv_count',\n",
        "            'serror_rate','srv_serror_rate','rerror_rate','srv_rerror_rate','same_srv_rate','diff_srv_rate','srv_diff_host_rate',\n",
        "            'dst_host_count','dst_host_srv_count','dst_host_same_srv_rate','dst_host_diff_srv_rate','dst_host_same_src_port_rate',\n",
        "            'dst_host_srv_diff_host_rate','dst_host_serror_rate','dst_host_srv_serror_rate','dst_host_rerror_rate',\n",
        "            'dst_host_srv_rerror_rate','attack','level'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "把 `KDDTrain+.txt`和 `KDDTest+.txt` 合并，方便进行数据预处理;\n",
        "\n",
        "若不合并，有以下几个方面的缺点：\n",
        "1. （最主要的）经过`get_dummies`函数处理之后，训练集和测试集的特征维度不一致，后续还需要人工处理成一致才能进行模型训练；\n",
        "\n",
        "2. 对训练集和测试集分别进行数据预处理，这样会增加代码量;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 合并训练集和测试集\n",
        "train = pd.read_csv(config['train'], names=columns, header=None)\n",
        "test = pd.read_csv(config['test'], names=columns, header=None)\n",
        "data = pd.concat([train, test], axis=0)\n",
        "data.to_csv(f'{project_home}/data/train_test.csv', index=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "给多类别属性OneHot编码"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "经过 `get_dummies`函数处理之后 'protocol_type', 'service', 'flag'，属性列被分成了多列，这些新的列的值都是0或1。原属性列会从数据集中删除。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_df = pd.get_dummies(data,\n",
        "                              columns=['protocol_type', 'service', 'flag'],\n",
        "                              prefix=\"\",\n",
        "                              prefix_sep=\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(148517, 124)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_df.shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "鉴于我们将训练集与测试集先合并再拆开，故两个数据集的OneHot编码结果是一样的，维度一致 "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "传闻`level`属性列，含有label的信息，为了防止label泄露，我们将其删除"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_df.drop('level', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['duration', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment',\n",
              "       'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised',\n",
              "       ...\n",
              "       'REJ', 'RSTO', 'RSTOS0', 'RSTR', 'S0', 'S1', 'S2', 'S3', 'SF', 'SH'],\n",
              "      dtype='object', length=123)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_df.columns"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DJb88PRGO8q7"
      },
      "source": [
        "## Dataset && DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "123"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(data_df.iloc[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['normal', 'neptune', 'warezclient', 'ipsweep', 'portsweep',\n",
              "       'teardrop', 'nmap', 'satan', 'smurf', 'pod', 'back',\n",
              "       'guess_passwd', 'ftp_write', 'multihop', 'rootkit',\n",
              "       'buffer_overflow', 'imap', 'warezmaster', 'phf', 'land',\n",
              "       'loadmodule', 'spy', 'perl', 'saint', 'mscan', 'apache2',\n",
              "       'snmpgetattack', 'processtable', 'httptunnel', 'ps', 'snmpguess',\n",
              "       'mailbomb', 'named', 'sendmail', 'xterm', 'worm', 'xlock',\n",
              "       'xsnoop', 'sqlattack', 'udpstorm'], dtype=object)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_df.attack.unique()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "把attack列放到最后一列"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['duration', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment',\n",
              "       'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised',\n",
              "       ...\n",
              "       'REJ', 'RSTO', 'RSTOS0', 'RSTR', 'S0', 'S1', 'S2', 'S3', 'SF', 'SH'],\n",
              "      dtype='object', length=123)"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "38"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# attack的下标为 38\n",
        "list(data_df.columns).index('attack')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "columns = list(data_df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "122"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 删除attack\n",
        "columns.pop(columns.index('attack'))\n",
        "len(columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 把attack放到最后一列，便于后续让最后一列作为label\n",
        "columns.append('attack')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_df = data_df[columns]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "attack已经切换到最后一列"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'attack'"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_df.columns[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_df.attack = data_df.attack.map(lambda x: 0 if x == 'normal' else 1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "attack属性列，已由字符串转换成数字 0 或 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0        0\n",
              "1        0\n",
              "2        1\n",
              "3        0\n",
              "4        0\n",
              "        ..\n",
              "22539    0\n",
              "22540    0\n",
              "22541    1\n",
              "22542    0\n",
              "22543    1\n",
              "Name: attack, Length: 148517, dtype: int64"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_df.attack"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MyDataset继承自Dataset，重写__getitem__和__len__方法，用于后续DataLoader的使用"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "5OVvyGfZtUHH"
      },
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "\n",
        "  def __init__(self, df):\n",
        "    self.df = df\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    data = self.df.iloc[idx,:-1]\n",
        "    label  = self.df.iloc[idx, -1] # 最后一列(attack)作为label\n",
        "    return torch.tensor(data).type(torch.float32), label\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.df.shape[0]\n",
        "\n",
        "data_dataset = MyDataset(data_df)\n",
        "train_dataset, val_dataset, test_dataset = random_split(data_dataset, [0.8, 0.1, 0.1])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(929, 117, 117)"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_loader), len(val_loader), len(test_loader)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fVD4U8GpPBb5"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`x = self.relu(x + self.cc4(x))`\n",
        "\n",
        "为残差网络的结构，可以有效防止梯度消失，提高模型的训练效果；\n",
        "使用残差网络在增加模型的深度同时使得模型具有更强的能力，但这样并不会导致模型的过拟合；\n",
        "\n",
        "正如我们知道模型的深度不是越深越好，而是要适当的深度，残差网络的出现，使得我们可以更加放心的增加模型的深度，而不用担心模型的过拟合；\n",
        "如果增加的这一层，没有起到作用，那么残差网络会自动地把这一层的权重逼近0，这样就不会影响模型的训练效果；就是 `self.cc4(x)` 的值为逼近0的值，这样就不会影响模型的训练效果；"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yX6Rg3M3MOo",
        "outputId": "c02105e8-e082-4448-98d0-6f981c5f7817"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Net(\n",
              "  (bn): BatchNorm1d(122, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (linear1): Linear(in_features=122, out_features=64, bias=True)\n",
              "  (cc1): Linear(in_features=64, out_features=64, bias=True)\n",
              "  (cc2): Linear(in_features=64, out_features=64, bias=True)\n",
              "  (cc3): Linear(in_features=64, out_features=64, bias=True)\n",
              "  (cc4): Linear(in_features=64, out_features=64, bias=True)\n",
              "  (cc5): Linear(in_features=64, out_features=64, bias=True)\n",
              "  (cc6): Linear(in_features=64, out_features=64, bias=True)\n",
              "  (linear2): Linear(in_features=64, out_features=2, bias=True)\n",
              "  (relu): ReLU()\n",
              ")"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class Net(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.bn = nn.BatchNorm1d(122)\n",
        "    mid = 64\n",
        "    self.linear1 = nn.Linear(122, mid)\n",
        "    self.cc1 = nn.Linear(mid, mid)\n",
        "    self.cc2 = nn.Linear(mid, mid)\n",
        "    self.cc3 = nn.Linear(mid, mid)\n",
        "    self.cc4 = nn.Linear(mid, mid)\n",
        "    self.cc5 = nn.Linear(mid, mid)\n",
        "    self.cc6 = nn.Linear(mid, mid)\n",
        "    self.linear2 = nn.Linear(mid, 2)\n",
        "    self.relu = nn.ReLU()\n",
        "  \n",
        "  # 残差网络\n",
        "  def forward(self, x):\n",
        "    x = self.bn(x)\n",
        "    x = self.relu(self.linear1(x))\n",
        "    x = self.relu(x + self.cc1(x))\n",
        "    x = self.relu(x + self.cc2(x))\n",
        "    x = self.relu(x + self.cc3(x))\n",
        "    x = self.relu(x + self.cc4(x))\n",
        "    x = self.relu(x + self.cc5(x))\n",
        "    x = self.relu(x + self.cc6(x))\n",
        "    x = self.linear2(x)\n",
        "    return x\n",
        "\n",
        "Net()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zUyI1hRrPIFE"
      },
      "source": [
        "## train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 在测试集上运行判断准确率\n",
        "@torch.no_grad()\n",
        "def predict(model, data_loader):\n",
        "  model.eval()\n",
        "  num = 0\n",
        "  right = 0\n",
        "  for loader in data_loader:\n",
        "    features = loader[0].to(device)\n",
        "    labels = loader[1].to(device)\n",
        "    num += len(labels)\n",
        "    y_hat = model(features).argmax(dim=-1)\n",
        "    batch_right = torch.sum(y_hat == labels).item()\n",
        "    right += batch_right\n",
        "  return right / num"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0jqMf_L8XqE",
        "outputId": "18bfe585-7938-41af-b281-be2633156c57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0\n",
            "acc: 0.8958035248371404 loss: 0.0020531468053963564\n",
            "val acc: 0.9525989765688123 val loss: 0.0038347863510119705\n",
            "test acc: 0.9512490741364218\n",
            "\n",
            "epoch 1\n",
            "acc: 0.9598868820172707 loss: 0.0008200583640026272\n",
            "val acc: 0.9602073794775114 val loss: 0.003411474112325754\n",
            "test acc: 0.9606087132179651\n",
            "\n",
            "epoch 2\n",
            "acc: 0.9682949820728197 loss: 0.00066381181662272\n",
            "val acc: 0.9679504443845947 val loss: 0.0035253631245319005\n",
            "test acc: 0.9684196350414114\n",
            "\n",
            "epoch 3\n",
            "acc: 0.9734206406652415 loss: 0.000571790750098654\n",
            "val acc: 0.9684217613789389 val loss: 0.002991055831552929\n",
            "test acc: 0.9698336812335869\n",
            "\n",
            "epoch 4\n",
            "acc: 0.975465854192267 loss: 0.0005237178929710836\n",
            "val acc: 0.9698357123619714 val loss: 0.003115694736688732\n",
            "test acc: 0.9709110497609589\n",
            "\n",
            "epoch 5\n",
            "acc: 0.9766609995455081 loss: 0.00048812380793412436\n",
            "val acc: 0.9709130083490439 val loss: 0.0030689245150459367\n",
            "test acc: 0.9718537472224092\n",
            "\n",
            "epoch 6\n",
            "acc: 0.9777383136667396 loss: 0.000458844711245173\n",
            "val acc: 0.9704416913546997 val loss: 0.002503662280097572\n",
            "test acc: 0.9720557538212915\n",
            "\n",
            "epoch 7\n",
            "acc: 0.9787651286885384 loss: 0.000443118769823999\n",
            "val acc: 0.9719229733369243 val loss: 0.0033605641618387923\n",
            "test acc: 0.9729984512827419\n",
            "\n",
            "epoch 8\n",
            "acc: 0.9795310316966014 loss: 0.0004196041565366344\n",
            "val acc: 0.9723942903312686 val loss: 0.0028747392729527722\n",
            "test acc: 0.9728637802168204\n",
            "\n",
            "epoch 9\n",
            "acc: 0.9800696887572171 loss: 0.00040739356216116666\n",
            "val acc: 0.9729329383248048 val loss: 0.002558183518069713\n",
            "test acc: 0.9728637802168204\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def train(train_loader, val_loader, epochs):\n",
        "  model = Net().to(device)\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(model.parameters(), lr=3e-4, momentum=0.9)\n",
        "  cur_val_loss = float('inf')\n",
        "  for i in range(epochs):\n",
        "    epoch_loss = 0\n",
        "    epoch_right = 0\n",
        "    nums = 0\n",
        "    model.train()\n",
        "    for loader in train_loader:\n",
        "      features = loader[0].to(device)\n",
        "      labels = loader[1].to(device)\n",
        "      optimizer.zero_grad()\n",
        "      y = model(features)\n",
        "      loss = loss_fn(y, labels)\n",
        "      loss.backward()\n",
        "      # 梯度裁剪预防梯度爆炸，限制模型参数的最大值为1\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "      epoch_loss += loss.item() / features.size(0)\n",
        "      # 每个bacth有512个，输出这512个中预测正确的个数\n",
        "      # print(torch.sum(y.argmax(dim=-1) == labels).item(), end=' ')\n",
        "      epoch_right += torch.sum(y.argmax(dim=-1) == labels).item()\n",
        "      nums += features.size(0)\n",
        "      optimizer.step()\n",
        "    print(f\"epoch {i}\\nacc: {epoch_right / nums} loss: {epoch_loss / len(train_loader)}\")\n",
        "    \n",
        "    # 在验证集上测试\n",
        "    val_loss = 0\n",
        "    val_right = 0\n",
        "    val_nums = 0\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for loader in val_loader:\n",
        "        features = loader[0].to(device)\n",
        "        labels = loader[1].to(device)\n",
        "        y = model(features)\n",
        "        loss = loss_fn(y, labels)\n",
        "        val_loss += loss.item() / features.size(0)\n",
        "        val_right += torch.sum(y.argmax(dim=-1) == labels).item()\n",
        "        val_nums += features.size(0)\n",
        "    print(f\"val acc: {val_right / val_nums} val loss: {val_loss / len(val_loader)}\")\n",
        "    print(f\"test acc: {predict(model, test_loader)}\")\n",
        "    print()\n",
        "\n",
        "    # 保存验证集损失最小的模型\n",
        "    if val_loss < cur_val_loss:\n",
        "      cur_val_loss = val_loss\n",
        "      torch.save(model.state_dict(), f\"model_{i}.pth\")\n",
        "  return model\n",
        "model = train(train_loader, val_loader, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Net(\n",
              "  (bn): BatchNorm1d(123, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (linear1): Linear(in_features=123, out_features=64, bias=True)\n",
              "  (cc1): Linear(in_features=64, out_features=64, bias=True)\n",
              "  (cc2): Linear(in_features=64, out_features=64, bias=True)\n",
              "  (cc3): Linear(in_features=64, out_features=64, bias=True)\n",
              "  (cc4): Linear(in_features=64, out_features=64, bias=True)\n",
              "  (cc5): Linear(in_features=64, out_features=64, bias=True)\n",
              "  (cc6): Linear(in_features=64, out_features=64, bias=True)\n",
              "  (linear2): Linear(in_features=64, out_features=2, bias=True)\n",
              "  (relu): ReLU()\n",
              ")"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "64QbgLRxPJ4y"
      },
      "source": [
        "## predict"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n",
        "epoch 6\n",
        "acc: 0.9777383136667396 loss: 0.000458844711245173\n",
        "val acc: 0.9704416913546997 val loss: 0.002503662280097572\n",
        "test acc: 0.9720557538212915\n",
        "```\n",
        "从第6个epoch开始，看到验证集模型的loss就不再下降了，所以我们为了避免过拟合，只训练6个epoch"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "其实个人在有些情况下会使用 `0.2 * train_loss + 0.8 * val_loss` 来判断loss不再下降的epoch数，这样可以更好的防止过拟合。因为如果只看 `val_loss`，这样可能会使得最终训练出来的模型，在验证集上的loss很小，但是在测试集上的loss很大，使得在验证集上过拟合了。\n",
        "\n",
        "但是个人发现，并没有其他人用过这种方式，故本人并不能说，这种方式一定能提高模型的效果。"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "不能根据test 测试数据集 的loss和acc来选择模型的权重，因为这样是作弊行为，同时也会导致模型的泛化能力下降；"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "接下来合并训练集和测试集，设置训练的epoch为6，重新训练模型，然后再进行预测；"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 合并训练集和验证集，使得更多的数据能够参与到模型的训练过程中，从而提高模型的泛化性\n",
        "train_val_dataset = ConcatDataset([train_dataset, val_dataset])\n",
        "train_val_loader = DataLoader(train_val_dataset, batch_size=config[\"batch_size\"], shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "def final_train(train_loader, epochs):\n",
        "  model = Net().to(device)\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(model.parameters(), lr=3e-4, momentum=0.9)\n",
        "  for i in range(epochs):\n",
        "    for loader in train_loader:\n",
        "      features = loader[0].to(device)\n",
        "      labels = loader[1].to(device)\n",
        "      optimizer.zero_grad()\n",
        "      y = model(features)\n",
        "      loss = loss_fn(y, labels)\n",
        "      loss.backward()\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "      optimizer.step()\n",
        "  return model\n",
        "model = final_train(train_val_loader, 6)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "将最终的模型参数保存到文件中，避免日后再训练，方便后续的预测使用；"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 默认注释保存模型的代码，只是为了防止不小心运行，因为已经保存了模型\n",
        "# torch.save(model, 'best_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model = torch.load('best_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9703050299643121"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict(best_model, test_loader)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "故我们认为我们的模型最终在测试集上的准确率为：0.97\n",
        "\n",
        "此时无论在测试集上的结果如何，都不要去再调了，不然会使得模型去拟合测试集，导致模型的泛化能力下降；"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
