{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "rUlX4frMW76D"
      ],
      "authorship_tag": "ABX9TyM83g6CjaamAyZDYwUx7gP2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JieShenAI/torch/blob/main/transformer/code/atttention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3r2F-4a3Dim6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "两个shape为(n,d)的矩阵相乘之后的结果矩阵，除以sqrt(n)之后的标准差才为1"
      ],
      "metadata": {
        "id": "hIz_Od97FM2q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sqrt(dk),dk就是两个矩阵相乘，消失的那个维度"
      ],
      "metadata": {
        "id": "HNOrHxyo8ZYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_attention_thita():\n",
        "  d = 10 ** 3\n",
        "  n = 10 ** 4\n",
        "  a = torch.randn(n,d)\n",
        "  print(\"a_var:\",a.var())\n",
        "  b = torch.randn(n,d)\n",
        "  print(\"b var:\",b.var())\n",
        "  res = torch.matmul(a,b.t()) / (d ** 0.5)\n",
        "  print(\"res.shape:\",res.shape,\"res.mean:\",res.mean(),\"res.std:\",res.var())\n",
        "get_attention_thita()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_O1UaaP4Dl_7",
        "outputId": "4c03ed1c-583a-4dc1-e112-7ad578605eff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a_var: tensor(1.0006)\n",
            "b var: tensor(1.0000)\n",
            "res.shape: torch.Size([10000, 10000]) res.mean: tensor(-2.2428e-05) res.std: tensor(1.0007)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "attention 通用函数"
      ],
      "metadata": {
        "id": "EXMNMaNKbRRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def attention(query, key, value):\n",
        "  d_k = query.size(-1)\n",
        "  scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "  p_attn = scores.softmax(dim=-1)\n",
        "  return torch.matmul(p_attn, value)"
      ],
      "metadata": {
        "id": "cOyUR7ABEibx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "decoder的掩码"
      ],
      "metadata": {
        "id": "-_kqBJG9Bp7w"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "id3m_ifOBsyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 框架"
      ],
      "metadata": {
        "id": "d2Nzvm4rHBC8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "transformer黑盒讲解: https://blog.csdn.net/zhaohongfei_358/article/details/126019181"
      ],
      "metadata": {
        "id": "W2Ztbg5YV5JL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### embedding的用法\n",
        "\n",
        "参考链接：https://blog.csdn.net/zhaohongfei_358/article/details/122809709"
      ],
      "metadata": {
        "id": "nfCBx7BkIBZK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "nn.Embedding中的参数会参与梯度下降的。更新模型参数也会更新nn.Embedding的参数，或者说nn.Embedding的参数本身也是模型参数的一部分。"
      ],
      "metadata": {
        "id": "BdFdZg3xMG3P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "nn.Embeddding接受两个重要参数：\n",
        "\n",
        "* num_embeddings：字典的大小。\n",
        "* embedding_dim：要将单词编码成多少维的向量\n",
        "\n",
        "例如，我们现在词典大小为20，现在要对hello, i, am，这三个单词进行编码，想将其编码为5维向量，则对应代码为："
      ],
      "metadata": {
        "id": "vIwoVjQhJi5J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "一般0表示句子开始(\\<bos>)，1表示句子结束(\\<eos>)，2为填充(\\<pad>)"
      ],
      "metadata": {
        "id": "-GfC26ajHDD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = nn.Embedding(20, 5, padding_idx=2) # 加上unknown对应的索引是2\n",
        "# 3,4,5 代表 hello,i,am\n",
        "token = embedding(torch.LongTensor([0,3,3,4,5,1,2,2]))\n",
        "token,token.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XF4E4ICJIEJ0",
        "outputId": "efd1103a-6a59-4594-e14b-43355605ac64"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-1.0356,  0.3127, -1.1603, -0.4334, -1.9822],\n",
              "         [-0.1711,  0.1356,  0.6236, -1.1335, -1.0227],\n",
              "         [-0.1711,  0.1356,  0.6236, -1.1335, -1.0227],\n",
              "         [ 1.3297, -0.1141,  1.3465,  1.1570,  1.0736],\n",
              "         [-0.9400,  0.4222, -1.2418,  1.7816,  2.1100],\n",
              "         [-0.1921, -0.3887,  0.9072, -1.1672, -0.9567],\n",
              "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
              "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
              "        grad_fn=<EmbeddingBackward0>),\n",
              " torch.Size([8, 5]))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding.weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qK3bmWuCM3DJ",
        "outputId": "dfc4042d-c213-45e3-b10c-705f980561f6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-1.0356,  0.3127, -1.1603, -0.4334, -1.9822],\n",
              "        [-0.1921, -0.3887,  0.9072, -1.1672, -0.9567],\n",
              "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
              "        [-0.1711,  0.1356,  0.6236, -1.1335, -1.0227],\n",
              "        [ 1.3297, -0.1141,  1.3465,  1.1570,  1.0736],\n",
              "        [-0.9400,  0.4222, -1.2418,  1.7816,  2.1100],\n",
              "        [ 1.3845, -0.0335,  0.4945, -2.1824, -0.0474],\n",
              "        [ 0.3450, -1.8370, -0.2142,  1.9349, -0.2536],\n",
              "        [ 0.7656, -1.7489, -0.9148,  0.5867, -0.6040],\n",
              "        [ 1.0515,  0.5338, -0.1564, -0.7515, -1.8969],\n",
              "        [ 0.2011, -0.2918,  1.2682, -0.4076, -1.9063],\n",
              "        [-0.4986,  0.3295, -0.1114, -0.0669, -0.1351],\n",
              "        [-0.2832, -0.8711, -0.6945,  1.8596, -0.0973],\n",
              "        [ 1.0646,  1.1381,  0.4273,  1.4794, -1.1820],\n",
              "        [ 0.9517, -1.1829, -2.3106, -0.7703,  0.3045],\n",
              "        [-0.2638,  0.7344, -1.9962, -1.0425, -1.7532],\n",
              "        [-1.5127,  0.0668,  2.4475, -0.4815, -0.0954],\n",
              "        [-0.6959,  0.7758,  1.2652,  0.1021,  1.4118],\n",
              "        [ 0.3007,  2.0941,  1.4253,  0.5476, -2.0132],\n",
              "        [-0.3591, -0.4900, -0.2637, -0.1045, -1.1299]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**相当于把输入的Tensor作为下标，从embedding的权重矩阵中取值**"
      ],
      "metadata": {
        "id": "m1Zhyj3uNHNR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "指定`padding_idx`后，embedding在`padding_idx`的权重为0"
      ],
      "metadata": {
        "id": "0DmwMN0pMoNb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Qa5KqPmoKDDW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### nn.Transformer"
      ],
      "metadata": {
        "id": "rUlX4frMW76D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "API文档: https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html"
      ],
      "metadata": {
        "id": "uH6sJYqdXG-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transformer_demo():\n",
        "  # 定义编码器，词典大小为10，要把token编码成128维的向量\n",
        "  embedding = nn.Embedding(10, 128)\n",
        "  # 定义transformer，模型维度为128（也就是词向量的维度）\n",
        "  transformer = nn.Transformer(d_model=128, batch_first=True) # batch_first一定不要忘记\n",
        "  # 定义源句子，可以想想成是 <bos> 我 爱 吃 肉 和 菜 <eos> <pad> <pad>\n",
        "  src = torch.LongTensor([[0, 3, 4, 5, 6, 7, 8, 1, 2, 2]])\n",
        "  \n",
        "  # 定义目标句子，可以想想是 <bos> I like eat meat and vegetables <eos> <pad>\n",
        "  tgt = torch.LongTensor([[0, 3, 4, 5, 6, 7, 8, 1, 2]])\n",
        "  \n",
        "  # jie: 我个人总觉得这个tgt应该是：\n",
        "  # tgt = torch.LongTensor([[3, 4, 5, 6, 7, 8, 1, 2, 2]])\n",
        "\n",
        "  # 将token编码后送给transformer（这里暂时不加Positional Encoding）\n",
        "  outputs = transformer(embedding(src), embedding(tgt))\n",
        "  f = lambda x: str(x.shape)\n",
        "  print(\"embedding(src).size: %s\\nembedding(tgt).size: %s\\noutputs.size: %s\" % (f(embedding(src)),f(embedding(tgt)),f(outputs)))\n",
        "        \n",
        "transformer_demo()\n"
      ],
      "metadata": {
        "id": "xpWjQfBdHCsC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "229197bd-11c8-4c5b-b247-4ae66fd58d4e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embedding(src).size: torch.Size([1, 10, 128])\n",
            "embedding(tgt).size: torch.Size([1, 9, 128])\n",
            "outputs.size: torch.Size([1, 9, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`nn.Transformer`输出的shape与embedding(tgt)的shape保持一致。"
      ],
      "metadata": {
        "id": "c1HUOfP9dqGe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "这个 `nn.Transformer`，如何使用损失函数?"
      ],
      "metadata": {
        "id": "BM-4YlZTSgX2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 掩码"
      ],
      "metadata": {
        "id": "0jIFvhSmde05"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "掩码如何使用?"
      ],
      "metadata": {
        "id": "d1aLR1PQbgsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nn.Transformer.generate_square_subsequent_mask(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfQu4PBmbiQG",
        "outputId": "1e472040-abbe-4cbd-f5b3-7fe463201fc3"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., -inf, -inf],\n",
              "        [0., 0., -inf],\n",
              "        [0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "验证掩码真的生效了。参考自: https://blog.csdn.net/zhaohongfei_358/article/details/122861751"
      ],
      "metadata": {
        "id": "hZ66PDnfeTIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def valid_mask():\n",
        "  embedding = nn.Embedding(10, 8)\n",
        "  # 定义Transformer，注意一定要改成eval模型，否则每次输出结果不一样\n",
        "  transformer = nn.Transformer(d_model=8, batch_first=True).eval()\n",
        "  src = torch.LongTensor([[0, 1, 2, 3, 4]]) # Encoder的输入\n",
        "  tgt = torch.LongTensor([[4, 3, 2, 5, 6, 7, 1, 0]]) # Decoder的输入\n",
        "\n",
        "  def mask_step(step):\n",
        "    return transformer(embedding(src), embedding(tgt[:, :step]),\n",
        "            # 这个就是用来生成阶梯式的mask的\n",
        "            tgt_mask=nn.Transformer.generate_square_subsequent_mask(step))\n",
        "\n",
        "  print(mask_step(3), mask_step(8), sep='\\n')\n",
        "  with torch.no_grad():\n",
        "    print(mask_step(8)[:,:3,:] - mask_step(3)) # 两值很接近\n",
        "\n",
        "valid_mask()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EbbSm7ibjfd",
        "outputId": "4346b45d-5396-42f9-97f7-d6e3d8bd9b84"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.8515, -1.2293,  0.6525,  1.0049, -0.8316, -1.6763,  0.4246,\n",
            "           0.8037],\n",
            "         [ 0.9800, -1.6200,  1.0930,  0.3331, -0.6457, -1.3456,  0.3236,\n",
            "           0.8815],\n",
            "         [ 0.8831, -1.5752,  0.3806,  0.3433, -0.6986, -1.3765,  0.9685,\n",
            "           1.0749]]], grad_fn=<NativeLayerNormBackward0>)\n",
            "tensor([[[ 0.8515, -1.2293,  0.6525,  1.0049, -0.8316, -1.6763,  0.4246,\n",
            "           0.8037],\n",
            "         [ 0.9800, -1.6200,  1.0930,  0.3331, -0.6457, -1.3456,  0.3236,\n",
            "           0.8815],\n",
            "         [ 0.8831, -1.5752,  0.3806,  0.3433, -0.6986, -1.3765,  0.9685,\n",
            "           1.0749],\n",
            "         [ 1.1004, -0.1711,  0.3520,  1.4578, -0.8099, -1.8714, -0.4478,\n",
            "           0.3899],\n",
            "         [ 0.9721, -0.7980,  0.8613,  0.9355, -0.8975, -1.7475, -0.2607,\n",
            "           0.9348],\n",
            "         [ 0.4392, -0.7488,  1.1428,  1.0083, -1.0944, -1.4566, -0.4676,\n",
            "           1.1771],\n",
            "         [ 0.4680, -1.2360,  1.0205,  1.2123, -1.1367, -1.3567,  0.3134,\n",
            "           0.7151],\n",
            "         [ 0.6018, -0.6733, -0.5036,  1.2430, -1.0533, -1.3778,  0.2400,\n",
            "           1.5232]]], grad_fn=<NativeLayerNormBackward0>)\n",
            "tensor([[[-1.1921e-07, -2.3842e-07,  2.3842e-07,  0.0000e+00,  5.9605e-08,\n",
            "           0.0000e+00,  4.7684e-07, -2.9802e-07],\n",
            "         [ 1.7881e-07,  2.3842e-07, -4.7684e-07, -1.7881e-07,  1.1921e-07,\n",
            "           1.1921e-07, -2.0862e-07,  0.0000e+00],\n",
            "         [ 1.1921e-07,  0.0000e+00, -2.9802e-08, -2.9802e-08, -1.1921e-07,\n",
            "           0.0000e+00,  1.1921e-07, -3.5763e-07]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8个token经过掩码的前3个的结果 与 只输入3个token经过掩码的结果 很接近"
      ],
      "metadata": {
        "id": "0df2lob9ibDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vdspFZ7VfABX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### transformer encoder"
      ],
      "metadata": {
        "id": "xEaNMwJ1ldve"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "veiO0D-6lh_S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}